{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "55661f4b",
      "metadata": {
        "id": "55661f4b"
      },
      "source": [
        "Some notes before running: because the checkpoint for torch's model is too large to push to GitHub, please download the saved checkpoint at [Google Drive](https://drive.google.com/file/d/1Eg4ZGp1hS-EcDB7LfCEUbPvtLzSjxc8f/view?usp=sharing) first and move it to the `work/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f08dc689",
      "metadata": {
        "id": "f08dc689"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.ml.classification import LogisticRegressionModel\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from classifiers import SENTIMENTS_AS_INDEX, MLPClassifierWithPhoBERT\n",
        "\n",
        "# Misc libs\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df7f9bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2df7f9bf",
        "outputId": "59998b87-c511-4276-e517-3e92f77cbf30"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "phobert_tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base-v2')\n",
        "apply_tokenization = lambda minibatch: phobert_tokenizer(\n",
        "    minibatch, return_tensors = 'pt', padding=True,\n",
        "    truncation=True, max_length=256\n",
        ")\n",
        "\n",
        "IDX_AS_SENT = {idx: sentiment for sentiment, idx in SENTIMENTS_AS_INDEX.items()}\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db203e46",
      "metadata": {
        "id": "db203e46"
      },
      "source": [
        "# Loading test data and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3c6329b7",
      "metadata": {
        "id": "3c6329b7"
      },
      "outputs": [],
      "source": [
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, data_as_spark_df):\n",
        "        self.data_as_rdd = data_as_spark_df.rdd.zipWithIndex()\n",
        "        self.len = data_as_spark_df.count()\n",
        "\n",
        "    def __len__(self): return self.len\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        if index < 0 or index > self.len - 1:\n",
        "            raise ValueError('index exceeded length of dataframe')\n",
        "\n",
        "        nth_row = (self.data_as_rdd\n",
        "                   .filter(lambda data: data[1] == index)\n",
        "                   .take(1)[0][0]\n",
        "        )\n",
        "        review, sentiment = nth_row\n",
        "\n",
        "        return review, SENTIMENTS_AS_INDEX[sentiment]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df14db6e",
      "metadata": {
        "id": "df14db6e"
      },
      "outputs": [],
      "source": [
        "test_df = spark.read.parquet(\n",
        "    'hdfs://namenode:9000/training_data/test_set'\n",
        ")\n",
        "test_set = ReviewDataset(test_df)\n",
        "test_loader = DataLoader(test_set, 512)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a6670a1",
      "metadata": {
        "id": "2a6670a1"
      },
      "source": [
        "## Loading Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9d517a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af9d517a",
        "outputId": "f5418d41-4221-4cf2-a767-5e04c9daf8e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "phobert = AutoModel.from_pretrained('vinai/phobert-base-v2')\n",
        "if torch.cuda.is_available: phobert.cuda()\n",
        "phobert.eval()\n",
        "\n",
        "@udf(returnType=VectorUDT())\n",
        "def create_embedding(text):\n",
        "    tokens = phobert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "    input_ids = tokens[\"input_ids\"].to(device)\n",
        "    attention_mask = tokens[\"attention_mask\"].to(device)\n",
        "    with torch.no_grad():\n",
        "        output = phobert(input_ids, attention_mask)\n",
        "    return Vectors.dense(output.last_hidden_state[0, 0, :].cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2c2b9f2",
      "metadata": {
        "id": "d2c2b9f2"
      },
      "outputs": [],
      "source": [
        "lr_model = LogisticRegressionModel.load('work/models/lr_sentiment_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71f86e02",
      "metadata": {
        "id": "71f86e02"
      },
      "source": [
        "## Loading MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6515e252",
      "metadata": {
        "id": "6515e252"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad\n",
        "def get_cm(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    n_labels: int,\n",
        "    use_gpu: bool = False,\n",
        "    return_preds: bool = False\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Make inference with a `torch.nn.Module` and return the confusion matrix.\n",
        "\n",
        "    :param nn.Module: The model to make inference with.\n",
        "    :param DataLoader: The data to make inference on.\n",
        "    :param int n_labels: The number of labels within the dataset. Note that\n",
        "        this should be the number of labels on the WHOLE dataset. The `data_loader`\n",
        "        must have at maximum `n_labels`.\n",
        "    :param bool use_gpu: Whether or not to do computations on GPU.\n",
        "    :param bool return_preds: Whether or not to return the predictions.\n",
        "\n",
        "    :return: A 2-d tensor of integers. Each row represents the predictions made and\n",
        "        each column represents the ground truth.\n",
        "\n",
        "        If `return_preds=True`, the function also returns the predictions.\n",
        "    :rtype: tuple[torch.Tensor, torch.Tensor]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    flattened_dim = n_labels ** 2\n",
        "    confusion_mat = torch.zeros(flattened_dim, dtype=torch.long)\n",
        "    preds = torch.empty(0)\n",
        "\n",
        "    for X, y in tqdm(data_loader):\n",
        "        tokenized_X = apply_tokenization(X)\n",
        "\n",
        "        X_input_ids = tokenized_X['input_ids']\n",
        "        X_att_mask = tokenized_X['attention_mask']\n",
        "\n",
        "        if use_gpu:\n",
        "            X_input_ids = X_input_ids.cuda()\n",
        "            X_att_mask = X_att_mask.cuda()\n",
        "\n",
        "        pred = model(X_input_ids, X_att_mask).argmax(dim=1).cpu()\n",
        "        if return_preds: preds = torch.concat([preds, pred])\n",
        "\n",
        "        count_as_idx = y + n_labels * pred\n",
        "        count_as_idx = torch.bincount(count_as_idx)\n",
        "        if count_as_idx.shape[0] < flattened_dim:\n",
        "            zeros = torch.zeros(flattened_dim - count_as_idx.shape[0], dtype=torch.long)\n",
        "            count_as_idx = torch.concat([count_as_idx, zeros])\n",
        "        confusion_mat += count_as_idx\n",
        "    return confusion_mat.reshape((n_labels, n_labels)), preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c270acaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c270acaa",
        "outputId": "e27d2793-84e1-40c9-fb4b-7a0c1a835445"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "checkpoint = torch.load('work/models/03_05_25-epoch25-model.tar', map_location=device)\n",
        "\n",
        "review_model = MLPClassifierWithPhoBERT([512, 512], nn.LeakyReLU(.02))\n",
        "review_model.load_state_dict(checkpoint['model_param'])\n",
        "if torch.cuda.is_available(): review_model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cea5b13",
      "metadata": {
        "id": "0cea5b13"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "356f7b54",
      "metadata": {
        "id": "356f7b54"
      },
      "source": [
        "## Comparision between Logistic Regression and MLP on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d51d7559",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d51d7559",
        "outputId": "af31938a-5a7d-472d-ebb5-cbeb0acd0a14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [03:20<00:00, 100.20s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[573,   7,   5],\n",
              "        [ 43,  14,  26],\n",
              "        [  6,   9, 121]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cm, mlp_predictions = get_cm(review_model, test_loader, 3, torch.cuda.is_available(), True)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4a2015ef",
      "metadata": {
        "id": "4a2015ef"
      },
      "outputs": [],
      "source": [
        "idx_to_sentiment = udf(lambda idx: IDX_AS_SENT[idx])\n",
        "\n",
        "test_df = test_df.withColumn('features', create_embedding(col('review')))\n",
        "lr_predictions = lr_model.transform(test_df)\n",
        "lr_predictions = lr_predictions.withColumn('prediction', idx_to_sentiment(col('prediction')))\n",
        "\n",
        "preds_pd = lr_predictions.select(\"sentiment\", \"prediction\").toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "AdcIJDMkIm4L",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdcIJDMkIm4L",
        "outputId": "1b3b7300-ef8c-4388-d81f-50c8555c6c93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP eval metrics per class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative     0.8897    0.7961    0.8403       152\n",
            "     neutral     0.1687    0.4667    0.2478        30\n",
            "    positive     0.9795    0.9212    0.9495       622\n",
            "\n",
            "    accuracy                         0.8806       804\n",
            "   macro avg     0.6793    0.7280    0.6792       804\n",
            "weighted avg     0.9323    0.8806    0.9026       804\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"MLP eval metrics per class:\")\n",
        "print(classification_report([IDX_AS_SENT[int(i)] for _, y in test_loader for i in y], [IDX_AS_SENT[int(i)] for i in mlp_predictions], digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "nTldbpo6E1H3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTldbpo6E1H3",
        "outputId": "7d559393-7118-4d61-ef98-33723d78209e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression eval metrics per class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative     0.8218    0.9408    0.8773       152\n",
            "     neutral     0.0000    0.0000    0.0000        30\n",
            "    positive     0.9665    0.9743    0.9704       622\n",
            "\n",
            "    accuracy                         0.9316       804\n",
            "   macro avg     0.5961    0.6384    0.6159       804\n",
            "weighted avg     0.9031    0.9316    0.9166       804\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Logistic Regression eval metrics per class:\")\n",
        "print(classification_report(preds_pd[\"sentiment\"], preds_pd[\"prediction\"], digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d505923",
      "metadata": {},
      "source": [
        "From the class-wise metrics, both models have relatively similar performance for `positive` and `negative` samples, shown by difference in F1 scores of around approximately 3%.\n",
        "\n",
        "Further inspection shows that MLP overall has higher precision in both `positive` and `negative` labels, but hinders in class-wise accuracy (lower recall values), with recall value in the `negative` class much lower than that of the LR model.\n",
        "\n",
        "Inspection on `neutral` samples shows that while MLP *can* correctly predict neutral reviews, its precision is rather low. Furthermore, the recall value of around 50% also shows that the model's ability for correctly predicting a neutral review is no better than random guessing.\n",
        "\n",
        "In summary, both models performs relatively well on positive and negative reviews, with almost equal performance. However, like most neural networks, MLP can generalize better for neutral reviews, shown by its ability to correctly predict some of the neutral reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190239d6",
      "metadata": {
        "id": "190239d6"
      },
      "source": [
        "## Inference on comments from Foody\n",
        "We picked out a few comments about KFC on [Foody](https://www.foody.vn/ho-chi-minh/kfc-ly-thuong-kiet/binh-luan) and use the models to do inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M3wAkq-MNTpO",
      "metadata": {
        "id": "M3wAkq-MNTpO"
      },
      "outputs": [],
      "source": [
        "with open('work/foody_reviews.txt') as file:\n",
        "    foody_reviews = file.read().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea23589b",
      "metadata": {
        "id": "ea23589b"
      },
      "outputs": [],
      "source": [
        "from pyvi import ViTokenizer\n",
        "# load dictionary\n",
        "abbr_fp = 'work/teencode.txt'\n",
        "\n",
        "ABBREV_DICT = {}\n",
        "with open(abbr_fp, 'r', encoding='utf-8') as abb:\n",
        "    for line in abb:\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) == 2:\n",
        "            short, full = parts\n",
        "            ABBREV_DICT[short] = ViTokenizer.tokenize(full)\n",
        "\n",
        "def clean_review(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Tokenize text with `PyVi.ViTokenizer` and subtitute abbreviation with its full form.\n",
        "    :param str text: The text to clean\n",
        "    :return: The cleaned text\n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "    return ' '.join(ABBREV_DICT.get(word.lower(), word) for word in ViTokenizer.tokenize(text).split(' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "C8yv2u-PNf-p",
      "metadata": {
        "id": "C8yv2u-PNf-p"
      },
      "outputs": [],
      "source": [
        "tokens = apply_tokenization(foody_reviews)\n",
        "mlp_predictions = review_model(tokens['input_ids'].cuda(), tokens['attention_mask'].cuda()).argmax(dim=1).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "3f112eb1",
      "metadata": {
        "id": "3f112eb1"
      },
      "outputs": [],
      "source": [
        "foody_rdd = spark.sparkContext.textFile('work/foody_reviews.txt').map(clean_review)\n",
        "foody_df = (foody_rdd.map(lambda x: Row(review=x))\n",
        "    .toDF()\n",
        "    .withColumn('features', create_embedding(col('review')))\n",
        ")\n",
        "lr_predictions = lr_model.transform(foody_df).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "8560d64e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8560d64e",
        "outputId": "54bd65b3-1bf0-4b08-9d26-ac0ba76f76bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tối ngày 10/10 mình có ghé KFC (chi nhánh*** ăn và gọi combo 1 gồm: 2 miếng gà rán, 1 khoai tây chiê...\n",
            "  MLP predicted: \u001b[31;10mnegative\u001b[0m\n",
            "  LR predicted: \u001b[31;10mnegative\u001b[0m\n",
            "\n",
            "KFC này mình hay ngồi lại ăn, không gian ổn, nhiều lần deli nên cũng tin tưởng chất lượng. Đi ban ng...\n",
            "  MLP predicted: neutral\u001b[0m\n",
            "  LR predicted: \u001b[32;10mpositive\u001b[0m\n",
            "\n",
            "Dịch vụ tệ, mình cảm thấy hơi có lỗi vì ghé gần giờ đóng cửa lúc 9: 25pm và 10:00 đóng cửa nhưng thự...\n",
            "  MLP predicted: \u001b[31;10mnegative\u001b[0m\n",
            "  LR predicted: \u001b[31;10mnegative\u001b[0m\n",
            "\n",
            "Chi nhánh này địa điểm đẹp, không gian rộng rãi, có lầu. Có điều đi 1 lần vào lúc 8h tối, nhân viên ...\n",
            "  MLP predicted: \u001b[31;10mnegative\u001b[0m\n",
            "  LR predicted: \u001b[31;10mnegative\u001b[0m\n",
            "\n",
            "Mình là một khách hàng quen thuộc của KFC, nhưng hôm nay KFC (cụ thể là KFC chi nhánh Lý...\n",
            "  MLP predicted: \u001b[31;10mnegative\u001b[0m\n",
            "  LR predicted: \u001b[31;10mnegative\u001b[0m\n",
            "\n",
            "Lần đầu trực tiếp qua ăn do gần nhà cũng có BigC vs Ng Tri Phương rồi. Mình rất thích bên này, ngoài...\n",
            "  MLP predicted: \u001b[32;10mpositive\u001b[0m\n",
            "  LR predicted: \u001b[32;10mpositive\u001b[0m\n",
            "\n",
            "Tôi vào quán lúc 12h50 phút ăn trưa. Vào order cơm trưa thì nhân viên (1 người mặc áo sơ mi trắng) b...\n",
            "  MLP predicted: \u001b[31;10mnegative\u001b[0m\n",
            "  LR predicted: \u001b[31;10mnegative\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ansi_colors = {\n",
        "    'positive': '\\033[32;10m',\n",
        "    'neutral': '',\n",
        "    'negative': '\\033[31;10m'\n",
        "}\n",
        "\n",
        "for i, review in enumerate(foody_reviews):\n",
        "  mlp_sent = IDX_AS_SENT[int(mlp_predictions[i])]\n",
        "  lr_sent = IDX_AS_SENT[int(lr_predictions.iloc[i]['prediction'])]\n",
        "  print(review[:100] + '...')\n",
        "  print(f\"  MLP predicted: {ansi_colors[mlp_sent]}{mlp_sent}\\033[0m\")\n",
        "  print(f\"  LR predicted: {ansi_colors[lr_sent]}{lr_sent}\\033[0m\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "313ff9ad",
      "metadata": {},
      "source": [
        "Both models mostly agree on reviews, except for the second review:\n",
        "```\n",
        "KFC này mình hay ngồi lại ăn, không gian ổn, nhiều lần deli nên cũng tin tưởng chất lượng.\n",
        "Đi ban ngày nên hơi nóng, có chỗ giữ xe có người canh. Gọi phần burger (combo) thêm cheese\n",
        "ăn khá ngon, nhưng mình gọi sớm nên chưa có tôm thì phải. Khoai tây ổn, nước oke nhưng gà\n",
        "dạo này hơi khô, trong lớp da nhiều mỡ ăn mau ngán. Phục vụ ổn.\n",
        "```\n",
        "\n",
        "While the review is indeed mostly positive, MLP can capture some context relating to criticism inside of the review, e.g. how the chicken was dry or was \"too fatty\" for their liking.\n",
        "\n",
        "Additionally, words like `\"ổn\"`, `\"oke\"` and `\"khá\"` are also typical of a 3-star review. Given that we have encoded 3-star review to be neutral, we consider this to be appriopriate."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "db203e46",
        "71f86e02"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".bigdata_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
