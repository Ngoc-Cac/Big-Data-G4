{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55661f4b",
   "metadata": {},
   "source": [
    "Some notes before running: because the checkpoint for torch's model is too large to push to GitHub, please download the saved checkpoint at [Google Drive](https://drive.google.com/file/d/1Eg4ZGp1hS-EcDB7LfCEUbPvtLzSjxc8f/view?usp=sharing) first and move it to the suitable directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08dc689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from classifiers import MLPClassifierWithPhoBERT, SENTIMENTS_AS_INDEX\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6329b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, data_as_spark_df):\n",
    "        self.data_as_rdd = data_as_spark_df.rdd.zipWithIndex()\n",
    "        self.len = data_as_spark_df.count()\n",
    "    \n",
    "    def __len__(self): return self.len\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if index < 0 or index > self.len - 1:\n",
    "            raise ValueError('index exceeded length of dataframe')\n",
    "        \n",
    "        nth_row = (self.data_as_rdd\n",
    "                   .filter(lambda data: data[1] == index)\n",
    "                   .take(1)[0][0]\n",
    "        )\n",
    "        review, sentiment = nth_row\n",
    "\n",
    "        return review, SENTIMENTS_AS_INDEX[sentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db203e46",
   "metadata": {},
   "source": [
    "# Loading test data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df14db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = spark.read.parquet(\n",
    "    'hdfs://namenode:9000/training_data/test_set'\n",
    ")\n",
    "test_set = ReviewDataset(test_set)\n",
    "test_loader = DataLoader(test_set, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f86e02",
   "metadata": {},
   "source": [
    "## Loading MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c270acaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('work/models/03_05_25-epoch25-model.tar', map_location=torch.device('cpu'))\n",
    "\n",
    "review_model = MLPClassifierWithPhoBERT([512, 512], nn.LeakyReLU(.02))\n",
    "review_model.load_state_dict(checkpoint['model_param'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "phobert_tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base-v2')\n",
    "apply_tokenization = lambda minibatch: phobert_tokenizer(\n",
    "    minibatch, return_tensors = 'pt', padding=True,\n",
    "    truncation=True, max_length=256\n",
    ")\n",
    "\n",
    "@torch.no_grad\n",
    "def get_cm(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    n_labels: int,\n",
    "    use_gpu: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Make inference with a `torch.nn.Module` and return the confusion matrix.\n",
    "\n",
    "    :param nn.Module: The model to make inference with.\n",
    "    :param DataLoader: The data to make inference on.\n",
    "    :param int n_labels: The number of labels within the dataset. Note that \n",
    "        this should be the number of labels on the WHOLE dataset. The `data_loader`\n",
    "        must have at maximum `n_labels`.\n",
    "    :param bool use_gpu: Whether or not to do computations on GPU.\n",
    "    :return: A 2-d tensor of integers. Each row represents the predictions made and\n",
    "        each column represents the ground truth.\n",
    "    :rtype: torch.Tensor\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    flattened_dim = n_labels ** 2\n",
    "    confusion_mat = torch.zeros(flattened_dim, dtype=torch.long)\n",
    "\n",
    "    for X, y in tqdm(data_loader):\n",
    "        tokenized_X = apply_tokenization(X)\n",
    "\n",
    "        X_input_ids = tokenized_X['input_ids']\n",
    "        X_att_mask = tokenized_X['attention_mask']\n",
    "\n",
    "        if use_gpu:\n",
    "            X_input_ids = X_input_ids.cuda()\n",
    "            X_att_mask = X_att_mask.cuda()\n",
    "\n",
    "        pred = model(X_input_ids, X_att_mask).argmax(dim=1).cpu()\n",
    "\n",
    "        count_as_idx = y + n_labels * pred\n",
    "        count_as_idx = torch.bincount(count_as_idx)\n",
    "        if count_as_idx.shape[0] < flattened_dim:\n",
    "            zeros = torch.zeros(flattened_dim - count_as_idx.shape[0], dtype=torch.long)\n",
    "            count_as_idx = torch.concat([count_as_idx, zeros])\n",
    "        confusion_mat += count_as_idx\n",
    "    return confusion_mat.reshape((n_labels, n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea5b13",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d7559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [05:08<00:00, 23.74s/it]\n"
     ]
    }
   ],
   "source": [
    "cm = get_cm(review_model, test_loader, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
