{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38b38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from classifiers import ReviewClassifierWithPhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b5c71ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1ad8495f25491d95c6284eb2bb8121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4298187ba841a0b89e488e74eb83e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2916fd03f54811a5ff5f35d286ff6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0941941590774a169b8780164ed69800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phobert_tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base-v2')\n",
    "apply_tokenization = lambda minibatch: phobert_tokenizer(\n",
    "    minibatch, return_tensors = 'pt', padding=True,\n",
    "    truncation=True, max_length=256\n",
    ")\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: nn.Module,\n",
    "    track_loss: bool = False,\n",
    "    use_gpu: bool = False\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Performs backpropogation on `model` using `optimizer`.\n",
    "\n",
    "    :param nn.Module model: The model on which to perform backpropogation.\n",
    "    :param nn.utils.data.DataLoader train_loader: A DataLoader dispatching batches\n",
    "        for each backpropogations.\n",
    "    :param nn.Module loss_fn: The loss function to based on which to compute gradients.\n",
    "    :param nn.Module optimizer: The optimization algorithm for gradient descent.\n",
    "    :param bool track_loss: Whether or not to return average loss.\n",
    "        This is `False` by default.\n",
    "\n",
    "    :return: A list of loss values per batch if `track_loss=True` else an empty list.\n",
    "    :rtype: list[float]\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_loader, start=1):\n",
    "        tokenized_X = apply_tokenization(X)\n",
    "        \n",
    "        X_input_ids = tokenized_X['input_ids']\n",
    "        X_att_mask = tokenized_X['attention_mask']\n",
    "\n",
    "        if use_gpu:\n",
    "            X_input_ids = X_input_ids.cuda()\n",
    "            X_att_mask = X_att_mask.cuda()\n",
    "            y = y.cuda()\n",
    "        pred_value = model(X_input_ids, X_att_mask)\n",
    "        loss = loss_fn(pred_value, y)\n",
    "\n",
    "        # Compute the gradient with loss.backward()\n",
    "        # Then backpropogate with optimizer.step()\n",
    "        # However, to avoid accumulation of previous backward passes\n",
    "        # we need to call optimizer.zero_grad() to zero out the gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if track_loss: total_loss += loss\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model(\n",
    "    model: nn.Module,\n",
    "    test_loader: DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    return_true_preds: bool,\n",
    "    use_gpu: bool = False\n",
    ") -> tuple[float, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Evaluate `model` based on `loss_fn` and return the average loss along with\n",
    "    true predictions and the total labels corresponding to each class.\n",
    "\n",
    "    :param nn.Module model: The model on which to perform evaluation.\n",
    "    :param nn.utils.data.DataLoader test_loader: A DataLoader containing test data.\n",
    "    :param nn.Module loss_fn: The loss function to based on which to compute metrics.\n",
    "    :param bool return_true_preds: Whether or not to store statistics on correctly\n",
    "        classified labels. This is only meaningful in the case the `model` is a classifier.\n",
    "\n",
    "    :return: The average loss (per batch). If `return_true_preds=True` then the number of\n",
    "        correctly classified labels and the total labels corresponding to each class are returned as\n",
    "        `torch.Tensor`. If not, zero tensors are returned instead.\n",
    "    :rtype: tuple[float, torch.Tensor, torch.Tensor]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    correct_labels = torch.tensor([0, 0, 0])\n",
    "    total_labels = torch.tensor([0, 0, 0])\n",
    "\n",
    "    for X, y in test_loader:\n",
    "        tokenized_X = apply_tokenization(X)\n",
    "\n",
    "        X_input_ids = tokenized_X['input_ids']\n",
    "        X_att_mask = tokenized_X['attention_mask']\n",
    "\n",
    "        if use_gpu:\n",
    "            X_input_ids = X_input_ids.cuda()\n",
    "            X_att_mask = X_att_mask.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        pred = model(X_input_ids, X_att_mask)\n",
    "        total_loss += loss_fn(pred, y)\n",
    "\n",
    "        if return_true_preds:\n",
    "            pred_labels = pred.argmax(dim=1)\n",
    "            correct_preds = pred_labels[pred_labels == y].bincount().cpu()\n",
    "            true_counts = y.bincount().cpu()\n",
    "\n",
    "            for i, count in enumerate(correct_preds):\n",
    "                correct_labels[i] += count\n",
    "            for i, count in enumerate(true_counts):\n",
    "                total_labels[i] += count\n",
    "\n",
    "    return total_loss / len(test_loader), correct_labels, total_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f44a3b5",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635ec70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').config('spark.ui.port', '4040').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31147f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      "\n",
      "Total reviews: 3425\n",
      "+--------------------+---------+\n",
      "|              review|sentiment|\n",
      "+--------------------+---------+\n",
      "|3 miếng gà 105k n...| negative|\n",
      "|Gà ướp vừa vị , m...| positive|\n",
      "|Thật tuyệt với gà...| positive|\n",
      "|Quán sạch , đẹp ,...| positive|\n",
      "|Nhân_viên bự con ...| positive|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_fp = 'hdfs://namenode:9000/review_data/preprocessed'\n",
    "preprocessed_df = spark.read.csv(preprocessed_fp, header=True, inferSchema=True)\n",
    "preprocessed_df = preprocessed_df.drop('rating', 'place_index')\n",
    "\n",
    "preprocessed_df.printSchema()\n",
    "print(f'Total reviews: {preprocessed_df.count()}')\n",
    "preprocessed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5cdd1a",
   "metadata": {},
   "source": [
    "## Buidling torch's Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24885502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    sentiment_as_index = {\n",
    "        'positive': 0,\n",
    "        'neutral': 1,\n",
    "        'negative': 2\n",
    "    }\n",
    "    def __init__(self, data_as_spark_df):\n",
    "        self.data_as_rdd = data_as_spark_df.rdd.zipWithIndex()\n",
    "        self.len = data_as_spark_df.count()\n",
    "    \n",
    "    def __len__(self): return self.len\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if index < 0 or index > self.len - 1:\n",
    "            raise ValueError('index exceeded length of dataframe')\n",
    "        \n",
    "        nth_row = (self.data_as_rdd\n",
    "                   .filter(lambda data: data[1] == index)\n",
    "                   .take(1)[0][0]\n",
    "        )\n",
    "        review, sentiment = nth_row\n",
    "\n",
    "        return review, ReviewDataset.sentiment_as_index[sentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3658a",
   "metadata": {},
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca7a6f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewDataset(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab2c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(dataset)),\n",
    "    test_size=.2, random_state=0,\n",
    "    stratify=[row['sentiment'] for row in preprocessed_df.collect()]\n",
    ")\n",
    "\n",
    "train_set, test_set = Subset(dataset, train_idx), Subset(dataset, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe9acf6",
   "metadata": {},
   "source": [
    "Compute the class weights for loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ebffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = preprocessed_df.groupBy('sentiment').count().collect()\n",
    "sentiment_weights = {}\n",
    "for class_, count in class_counts:\n",
    "    sentiment_weights[class_] = len(dataset) / (count * len(class_counts))\n",
    "\n",
    "sentiment_weights = torch.tensor([sentiment_weights[class_] for class_ in dataset.sentiment_as_index], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ced0b0",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "980b1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def run_epochs(\n",
    "    epochs: int,\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: nn.Module, *,\n",
    "    update_rate: int | None = None\n",
    "):\n",
    "    num_dig = int(math.log10(epochs)) + 1\n",
    "    if update_rate is None:\n",
    "        update_rate = 1 if epochs <= 20 else 10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if not epoch % update_rate:\n",
    "            print(f\"\\033[102;30;1mEpoch {epoch + 1:>{num_dig}}/{epochs}\\033[0m\", end=' || ')\n",
    "\n",
    "        training_loss = train_model(\n",
    "            model, train_loader,\n",
    "            loss_fn, optimizer, track_loss=True,\n",
    "            use_gpu=torch.cuda.is_available()\n",
    "        )\n",
    "        if not epoch % update_rate:\n",
    "            print(f\"\\033[94;1mTraining loss: {training_loss * 100:<10.6f}\\033[0m\", end=' | ')\n",
    "\n",
    "        loss, true_labels, total_labels = test_model(\n",
    "            model, test_loader, loss_fn, True,\n",
    "            use_gpu=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        if not epoch % update_rate:\n",
    "            acc_by_class = (true_labels / total_labels) * 100\n",
    "            avg_acc = (true_labels.sum() / total_labels.sum()) * 100\n",
    "            print(f\"\"\"\\033[94;1mEval Loss: {loss:<10.6f}\\033[0m\n",
    "  Average Accuracy: {avg_acc:.4f}%\n",
    "  Pos: {acc_by_class[0]:<7.4f}% | Neu: {acc_by_class[1]:<7.4f}% | Neg: {acc_by_class[2]:<7.4f}%\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc7cff28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe18ac37d31541cbb9176ed224f5d12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d63765ce7949a3846113a921fb6897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.00005\n",
    "\n",
    "trainloader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_set, batch_size, shuffle=True)\n",
    "\n",
    "review_model = ReviewClassifierWithPhoBERT()\n",
    "if torch.cuda.is_available():\n",
    "    sentiment_weights = sentiment_weights.cuda()\n",
    "    review_model.cuda()\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss(weight=sentiment_weights)\n",
    "optimizer = torch.optim.Adam(review_model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fa5c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[102;30;1mEpoch 1/1\u001b[0m || \u001b[94;1mTraining loss: 100.860954\u001b[0m | \u001b[94;1mEval Loss: 0.874189  \u001b[0m\n",
      "  Average Accuracy: 84.8175%\n",
      "  Pos: 87.4317% | Neu: 0.0000 % | Neg: 89.3805%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52252)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "run_epochs(\n",
    "    epochs, review_model,\n",
    "    trainloader, testloader,\n",
    "    cross_entropy, optimizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
