{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e62e24b8",
   "metadata": {},
   "source": [
    "# Crawling to your host machine\n",
    "This is relatively straight-forward. You save the results to a file on your local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920dcd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ggplace_review_crawler import ReviewCrawler\n",
    "\n",
    "urls = []\n",
    "\n",
    "metadata_file = open('../docker-hadoop/data-mount/place_meta.csv', 'w', encoding='utf-8')\n",
    "reviews_file = open('../docker-hadoop/data-mount/reviews.csv', 'w', encoding='utf-8')\n",
    "crawler = ReviewCrawler()\n",
    "\n",
    "metadata_file.write('place_index,address,price_range')\n",
    "reviews_file.write('review,rating,place_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0210323",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.open()\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    data = crawler.crawl_from(url)\n",
    "\n",
    "    metadata_file.write(f\"{i},{data['address']},{data['price_range']}\")\n",
    "    for review, rating in zip(data['reviews'], data['ratings']):\n",
    "        review = review.replace('\\n ', '. ').replace('\"', \"'\")\n",
    "        reviews_file.write(f'\"{review}\",{rating},{i}\\n')\n",
    "\n",
    "# don't forget to close these\n",
    "crawler.close()\n",
    "metadata_file.close()\n",
    "reviews_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d3690",
   "metadata": {},
   "source": [
    "# Crawling to an hosted HDFS on Docker\n",
    "If you are hosting a HDFS locally on your machine. It should be straight-forward, just establish a client like below (after changing the IP address of course) and write to file on the HDFS.\n",
    "\n",
    "However, if you are hosting the HDFS through a Docker container, it will be a bit more contrived. Essentially, you can create a client the same way, but, you need to expose the datanode's 9864 port for the client to do any data transfer.\\\n",
    "On the other hand, you may also run your Python scripts within the Docker host so you don't need to expose the ports, which is what being done in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db9988",
   "metadata": {},
   "source": [
    "## How does this work?\n",
    "The project hosts a HDFS within a Docker application. The application consists of multiple containers, these containers have the corresponding Hadoop service, Spark service and Jupyter service with PySpark installed.\n",
    "\n",
    "When the aplication is built and run, a Jupyter server is hosted locally and one may connect to this server to run Python scripts.\n",
    "\n",
    "## What to do now?\n",
    "Before doing anything within this notebook, you must first upload the [`ggplace_review_crawler`](./ggplace_review_crawler/) package to the Jupyter server. You can either do this by copying everything into the appropriate container or you may do this in the WebUI accessible through http://localhost:8888.\n",
    "\n",
    "After that, you may run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary libraries\n",
    "%pip install selenium\n",
    "%pip install hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import Client\n",
    "from ggplace_review_crawler import ReviewCrawler\n",
    "\n",
    "client = Client('http://namenode:9870')\n",
    "crawler = ReviewCrawler()\n",
    "\n",
    "client.write('/review_data/place_meta.csv', data='place_index,address,price_range')\n",
    "client.write('/review_data/reviews.csv', data='review,rating,place_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.open()\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    data = crawler.crawl_from(url)\n",
    "\n",
    "    client.write('/review_data/place_meta.csv',\n",
    "                 data=f\"{i},{data['address']},{data['price_range']}\",\n",
    "                 encoding='utf-8',\n",
    "                 append=True\n",
    "    )\n",
    "\n",
    "    review_data = ''.join([\n",
    "        f'\"{review.replace('\\n ', '. ').replace('\"', \"'\")}\",{rating},{i}\\n'\n",
    "        for review, rating in zip(data['reviews'], data['ratings'])\n",
    "    ])\n",
    "    client.write('/review_data/reviews.csv',\n",
    "                 data=review_data,\n",
    "                 encoding='utf-8',\n",
    "                 append=True\n",
    "    )\n",
    "\n",
    "# don't forget to close\n",
    "crawler.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
