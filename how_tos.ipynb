{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b3fe7b",
   "metadata": {},
   "source": [
    "# Who is this notebook for?\n",
    "This notebook is meant for those who want to test out the functionalities mentioned in the project without the hassle of installing and setting up Java, Apache Hadoop and Apache Spark.\\\n",
    "If you already have installed and set up, or will be installing and setting up, Hadoop and Spark on your machine, you don't need to set up the Docker application anymore. In such case, just disregard this notebook.\n",
    "\n",
    "**Table of contents**\n",
    "- [**Running the Docker Application**](#running-the-docker-application)\n",
    "- [**Using the Docker Application**](#using-the-docker-application)\n",
    "    - [Accessing the HDFS](#accessing-the-hdfs)\n",
    "    - [Executing Python scripts within the application](#executing-python-scripts-within-the-application)\n",
    "    - [Accessing HDFS with Python](#accessing-hdfs-with-python)\n",
    "    - [Accessing HDFS from PySpark Session](#accessing-hdfs-from-pyspark-session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4af52e",
   "metadata": {},
   "source": [
    "# **Running the Docker Application**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf33605",
   "metadata": {},
   "source": [
    "## Installing Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4111d84",
   "metadata": {},
   "source": [
    "With that in mind, the first thing you need to do is installing [Docker](https://www.docker.com/) if you haven't. For Windows, Mac and Linux, you can install Docker through [Docker Desktop](https://docs.docker.com/get-started/get-docker/). For Linus, you can also install the [Docker Engine](https://docs.docker.com/engine/install/) without having to install the Docker Desktop UI.\\\n",
    "***Note that you may need to restart your machine after installation***\n",
    "\n",
    "After installation and setting up Docker, you can test it out by deploying some prebuilt images. If you are completely new to Docker, you can visit the [Introduction course](https://docs.docker.com/get-started/introduction/) after installation to test Docker out. For the project, we will be working with these, included but not limited to, Docker concepts:\n",
    "- [Images](https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-an-image/) and [Containers](https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-container/).\n",
    "- [Publishing ports](https://docs.docker.com/get-started/docker-concepts/running-containers/publishing-ports/) of containers.\n",
    "- Sharing files between host and containers by [bind mounting](https://docs.docker.com/get-started/docker-concepts/running-containers/sharing-local-files/).\n",
    "- Running multi-container application with [Docker Compose](https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-docker-compose/) file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7f10b",
   "metadata": {},
   "source": [
    "## Deploying the application on your host"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f1b55",
   "metadata": {},
   "source": [
    "When everything is set, you can finally get started by deploying the applciation on your host machine.\\\n",
    "In your CLI of choice, navigate to the [`docker-hadoop/`](./docker-hadoop/) directory and run:\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "You should see the following output:\n",
    "\n",
    "![compose_output](./resource/demo/compose_output.png)\n",
    "\n",
    "If this is your first time building the services, you will need to wait for Docker to pull the images to your machine, which may take up to 5 minutes depending on your connection speed.\n",
    "\n",
    "When everything finishes, you can check the status of running containers by executing `docker-compose ps` or `docker ps`. If you have installed Docker Desktop, you can also see the status reported in the Containers tab.\n",
    "\n",
    "![status_cli](./resource/demo/status_cli.png)\n",
    "![status_ui](./resource/demo/status_ui.png)\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "*Side note*: When you are done with the current session and want to terminate the application, run the following command:\n",
    "```bash\n",
    "docker-compose stop\n",
    "```\n",
    "This will stop the running containers and you can start the container again with `docker-compose start`.\n",
    "\n",
    "However, if you want to completely remove the containers and networks created by the `docker-compose.yml` file, run:\n",
    "```bash\n",
    "docker-compose down\n",
    "```\n",
    "To also remove the associated volumes, specify the additional option `-v`.\n",
    "\n",
    "For more information about the `docker compose` command, see [here](https://docs.docker.com/reference/cli/docker/compose/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20066326",
   "metadata": {},
   "source": [
    "# **Using the Docker Application**\n",
    "After everything is successfully deployed, you can use the application as is, you may also modify the `docker-compose.yml` file to better suit your needs.\n",
    "\n",
    "The following section demonstrate some of the functionalities available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45223a5",
   "metadata": {},
   "source": [
    "# **Accessing the HDFS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf223f8",
   "metadata": {},
   "source": [
    "## Within the bash shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866774f9",
   "metadata": {},
   "source": [
    "One way to interact with the HDFS hosted by the application is through the bash shell of the namenode container. To enter the container's bash shell, run:\n",
    "```bash\n",
    "docker exec -it namenode bash\n",
    "```\n",
    "Note that the namenode container must be running in order to do this.\n",
    "\n",
    "![namenode_bash](./resource/demo/namenode_bash.png)\n",
    "\n",
    "Within this container, you can access the HDFS by using `hdfs dfs` command. Run `hdfs dfs -help` to see available commands and their options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb54e720",
   "metadata": {},
   "source": [
    "## Within the WebHDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd6749",
   "metadata": {},
   "source": [
    "By default, the Docker application is set up with WebHDFS and the 9870 UI port has been exposed to the same port in host for access outside of the container. To access the UI, just visit http://localhost:9870.\n",
    "\n",
    "Within the web UI, you can view the HDFS by going to `Browse the file system` option inside the `Utilities` tab. Here you can interactively create or delete directories and files.\n",
    "<div align='center'>\n",
    "    <img src=\"./resource/demo/file_browse.png\">\n",
    "    <img src=\"./resource/demo/webhdfs.png\" width=\"660\">\n",
    "</div>\n",
    "\n",
    "However, you will run into an error when uploading files from your local file system, this is because the WebHDFS is trying to do data transfer through http://datanode:9864 which is non-existent outside of the Docker application environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b176ad05",
   "metadata": {},
   "source": [
    "# **Executing Python scripts within the application**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79528a",
   "metadata": {},
   "source": [
    "As mentioned above, aside from the HDFS service, the application also provide a Jupyter service with PySpark pre-installed.\\\n",
    "This service uses Python 3.11.6 and allow you to run any Python scripts of choice within the container and even establish external connection to the Jupyter Server outside of the Docker environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b993b",
   "metadata": {},
   "source": [
    "## Entering the container to execute Python scripts\n",
    "In order to run Python scripts within the container, you must first enter into its bash shell like before:\n",
    "```bash\n",
    "docker exec -it docker-hadoop-spark-notebook-1 bash\n",
    "```\n",
    "When entered, you will be under the username '**joyvan**'. This is the default user of the image. Within the bash shell, you can invoke Python with `python`. For example, checking the current version of the installed Python executable.\n",
    "\n",
    "![py_ver](./resource/demo/py_ver.png)\n",
    "\n",
    "Likewise, to execute a Python file, you can run `python path/to/file.py`; or to use pip install, you can run `python -m pip install library`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4eaba",
   "metadata": {},
   "source": [
    "## Using the Jupyter service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce703d",
   "metadata": {},
   "source": [
    "Because of the nature of the project, you might also want to run an Interactivate Python session with Jupyter notebooks.\n",
    "\n",
    "You have two options of doing this:\n",
    "- Either by using the Web UI\n",
    "- Or connecting to the server from your own IDE that supports Jupyter notebooks.\n",
    "\n",
    "**For the accessing the Web UI**: just visit http://localhost:8888. When arrived, you can freely create noteboooks, Python scripts and other files. You may also upload files from your local file system.\n",
    "\n",
    "**For connecting to the Jupyter Server**:\n",
    "1. In your Jupyter notebook, when selecting a kernel, choose the option `Existing Jupyter Server...`. This may be locked behind the `Select Another Kernel...` option.\n",
    "2. When selected, you will be prompted to enter the URL of the Jupyter Server, use the same URL mentioned above.\n",
    "3. After entering the URL, you will then be asked whether you want to connect to an insecure server, select `Yes`. \n",
    "4. After that, you can change your display name of the server and then connect to the Python 3 kernel.\n",
    "\n",
    "***On remarks of the insecure connection***: This is because we have set up the server to be **passwordless** as well as requiring **no** token authentication. We are hosting the server locally and do not have any intention of sharing access, so there was no need of authentication. If you intend to share access with someone, it is recommended to have at least set up token authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcae479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have successfully connected to the server, you can test out the following lines\n",
    "print('This is some text to print out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65050e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also send some of the command to the bash shell\n",
    "!ls\n",
    "# or maybe create some file\n",
    "!echo '' > just_some_text.txt\n",
    "!head just_some_text.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080a738",
   "metadata": {},
   "source": [
    "# **Accessing HDFS with Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d6ec52",
   "metadata": {},
   "source": [
    "As mentioned in [Accessing the HDFS](#within-the-webhdfs), you can't upload files from your local file system because you are essentially trying to do data transfer through the address http://datanode:9864 which is non-existent outside of the application. The same thing will happen if you try to access the HDFS through a client like the one provided by [hdfs](https://pypi.org/project/hdfs/). You can establish a connection to http://localhost:9870, but you will not be able to do any read/write operations to the files on HDFS.\n",
    "\n",
    "For automation tasks that require saving directly to the HDFS, we must do this within the Docker application.\n",
    "\n",
    "To see some demonstrations, run the following cells after connecting to the Jupyter Server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9983a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import Client\n",
    "client = Client('http://namenode:9870') # the client needs to connect to the WebHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list('/') # all path begins with /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45690d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.makedirs('/demo')\n",
    "client.list('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc01d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/demo/demo.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!echo \"let's make a text file and put some words into it\" > demo.txt\n",
    "client.upload('/demo/demo.txt', 'demo.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9bc1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demo.txt']\n",
      "b\"let's make a text file and put some words into it\\n\"\n"
     ]
    }
   ],
   "source": [
    "print(client.list('/demo'))\n",
    "with client.read('/demo/demo.txt') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b39271b",
   "metadata": {},
   "source": [
    "# **Accessing HDFS from PySpark Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97de7c",
   "metadata": {},
   "source": [
    "To read or write to a file on the HDFS with PySpark, you just simply specify the path as `hdfs://namenode:9000/path/to/file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d7da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a51759eb2de0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe9e9b8db50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fedd5fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"let's make a text file and put some words into it\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_prev_txt = sc.textFile('hdfs://namenode:9000/demo/demo.txt')\n",
    "the_prev_txt.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
